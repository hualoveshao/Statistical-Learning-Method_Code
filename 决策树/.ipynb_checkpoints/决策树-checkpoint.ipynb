{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 (9000, 965)\n",
      "测试集 (1000, 965)\n"
     ]
    }
   ],
   "source": [
    "#文件预处理成tsv格式\n",
    "def read_file(file_name):\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "    label_map = {}\n",
    "    label_map[\"ham\"] = 0\n",
    "    label_map[\"spam\"] = 1\n",
    "    \n",
    "    \n",
    "    text = list()\n",
    "    label = list()\n",
    "    label_num = list()\n",
    "    for item in data:\n",
    "        item = item.strip()\n",
    "        item = item.split(\" \")\n",
    "        item_len = len(item)\n",
    "        tmp_list = list()\n",
    "        if(item[1] in [\"ham\",\"spam\"]):\n",
    "            label.append(item[1])\n",
    "            label_num.append(label_map[item[1]])\n",
    "            for index in range(2,item_len,2):\n",
    "                tmp_list.append(item[index])\n",
    "        text.append(tmp_list)\n",
    "    return text,label,label_num\n",
    "\n",
    "\n",
    "train_text,train_label,train_label_num = read_file(\"../data/train\")\n",
    "test_text,test_label,test_label_num = read_file(\"../data/test\")\n",
    "\n",
    "\n",
    "train_text = [\" \".join(item) for item in train_text]\n",
    "test_text = [\" \".join(item) for item in test_text]\n",
    "\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "part_fit = cv.fit(train_text) # 以部分句子为参考\n",
    "train_all_count = cv.transform(train_text) # 对训练集所有邮件统计单词个数\n",
    "test_all_count = cv.transform(test_text) # 对测试集所有邮件统计单词个数\n",
    "tfidf = TfidfTransformer()\n",
    "train_tfidf_matrix = tfidf.fit_transform(train_all_count)\n",
    "test_tfidf_matrix = tfidf.fit_transform(test_all_count)\n",
    "\n",
    "\n",
    "\n",
    "print('训练集', train_tfidf_matrix.shape)\n",
    "print('测试集', test_tfidf_matrix.shape)\n",
    "\n",
    "#训练数据\n",
    "train_tfidf_matrix_1 = train_tfidf_matrix.toarray()\n",
    "#测试数据\n",
    "test_tfidf_matrix1 = test_tfidf_matrix.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 113)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=0.5)# 保证降维后的数据保持90%的信息\n",
    "pca.fit(train_tfidf_matrix_1)\n",
    "pca_train_tfidf_matrix_1 = pca.transform(train_tfidf_matrix_1)\n",
    "pca_train_tfidf_matrix_1.shape\n",
    "pca_test_tfidf_matrix_1 = pca.transform(test_tfidf_matrix1)\n",
    "print(pca_train_tfidf_matrix_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 分析：\n",
    "0. 处理数据排序\n",
    "1. 建立决策树\n",
    "2. 根据决策树进行预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预处理数据 空间换时间\n",
    "#输入处理好的训练数据，输出是每个特征均进行排序，\n",
    "#且train_label变成特征相同的列数\n",
    "def sub_tree(train_data,train_label,segment_x):\n",
    "    return train_data[:segment_x+1],train_label[:segment_x+1],train_data[segment_x+1:],train_label[segment_x+1:]\n",
    "#获取信息增益比i是列，j是行\n",
    "def get_ratio(i,j,train_data,train_label):\n",
    "    m = train_data.shape[0]\n",
    "    s_one = train_label[:j+1].sum()\n",
    "    b_one = train_label[j+1:].sum()\n",
    "    a = (j+1)/m*(1-pow(s_one/(j+1),2)*pow((j+1-s_one)/(j+1),2))\n",
    "    b = (m-j-1)/m*(1-pow(b_one/(m-j-1),2)*pow((m-j-1-b_one)/(m-j-1),2))\n",
    "    return a+b\n",
    "    \n",
    "      \n",
    "    \n",
    "\n",
    "'''\n",
    "    输入：训练数据\n",
    "    输出：对应信息增益比最大的特征和特征中的分割点\n",
    "'''\n",
    "def calculate_f_s_i(train_data,train_label):\n",
    "    max_ratio = 100\n",
    "    max_feature = 0\n",
    "    segment_point = 0\n",
    "    segment_x = 0\n",
    "    tmp_data = train_data\n",
    "    tmp_label = train_label\n",
    "    n = train_data.shape[1]\n",
    "    m = train_data.shape[0]\n",
    "    for i in (range(n)):\n",
    "        #按照列排序\n",
    "        index = np.argsort(train_data[:,i])\n",
    "        train_label = np.array([train_label[inx] for inx in index])\n",
    "        train_data = np.array([train_data[inx] for inx in index])\n",
    "        for j in range(m-1):\n",
    "            ratio = get_ratio(i,j,train_data,train_label)\n",
    "            if(ratio<max_ratio):\n",
    "                max_ratio = ratio\n",
    "                max_feature = i\n",
    "                segment_point = (train_data[j][i]+train_data[j+1][i])/2\n",
    "                segment_x = j\n",
    "                tmp_data = train_data\n",
    "                tmp_label = train_label\n",
    "    return max_feature,segment_point,max_ratio,segment_x,tmp_data,tmp_label\n",
    "\n",
    "def Major_class(train_label):\n",
    "    s = train_label.sum()\n",
    "    if(s>len(train_label)/2):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "'''\n",
    "    输入训练数据和标签，递归返回一个树的结构，每次均是二叉树，连续数据的中间分类\n",
    "    tree[分割特征][分割浮点数][小于浮点数 or 大于浮点数]\n",
    "    1. 获取信息增益比最大的特征和分割点\n",
    "    2. 判断是否可以结束程序\n",
    "    3. 根据分割的特征和分割点进入下一层树，之前需要对数据进行分割\n",
    "'''\n",
    "def create_tree(train_data,train_label,depth):\n",
    "    #获取信息增益比最大的特征、分割点和对应的信息增益比\n",
    "    feature,segment_point,inf_gain_ratio,segment_x,tmp_data,tmp_label = calculate_f_s_i(train_data,train_label)\n",
    "    #结束条件\n",
    "    train_label_dict = {i for i in train_label}\n",
    "    if(len(train_label_dict)==1):\n",
    "        return train_label[0]\n",
    "    #深度剪枝\n",
    "    if(depth>=16):\n",
    "        return Major_class(train_label)\n",
    "    #广度剪枝\n",
    "#     if(train_data.shape[0]<5):\n",
    "#         return Major_class(train_label)\n",
    "    \n",
    "    #基尼系数剪枝\n",
    "    #进入下一层，需要定义一个分割函数\n",
    "    result = {feature:{segment_point:{}}}\n",
    "    a,b,c,d = sub_tree(tmp_data,tmp_label,segment_x)\n",
    "    result[feature][segment_point][0] = create_tree(a,b,depth+1)\n",
    "    result[feature][segment_point][1] = create_tree(c,d,depth+1)\n",
    "    return result\n",
    "\n",
    "#测试数据\n",
    "# result = {feature:{segment_point:{}}}\n",
    "def get_test_result(test_data,tree):\n",
    "    m,n = test_data.shape[0],test_data.shape[1]\n",
    "    result_list = list()\n",
    "    tmp = tree\n",
    "    for i in range(m):\n",
    "        single_test = test_data[i]\n",
    "        \n",
    "        while True:\n",
    "            if(type(tree).__name__ != 'dict'):\n",
    "                result_list.append(tree)\n",
    "                break\n",
    "            (key, value), = tree.items()\n",
    "            dataVal = single_test[key]\n",
    "            (k,v), = value.items()\n",
    "            if(dataVal<k):\n",
    "                tree = v[0]\n",
    "            else:\n",
    "                tree = v[1]\n",
    "        tree = tmp\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.889"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.889\n",
      "0.823\n",
      "0.801\n",
      "0.851\n",
      "0.844\n",
      "0.838\n",
      "0.846\n",
      "0.843\n",
      "0.835\n",
      "0.831\n",
      "0.849\n",
      "0.843\n",
      "0.818\n",
      "0.824\n",
      "0.845\n",
      "0.808\n",
      "0.848\n",
      "0.795\n",
      "0.607\n",
      "0.814\n"
     ]
    }
   ],
   "source": [
    "result_list = list()\n",
    "for i in range(200):\n",
    "    split_num = 40\n",
    "    pca_train_tfidf_matrix_2 = pca_train_tfidf_matrix_1[i*split_num:(i+1)*split_num]\n",
    "    train_label_num2 = train_label_num[i*split_num:(i+1)*split_num]\n",
    "    result = create_tree(pca_train_tfidf_matrix_2,train_label_num2,0)\n",
    "    test_result = get_test_result(pca_test_tfidf_matrix_1,result)\n",
    "    print(accuracy_score(test_result,test_label_num))\n",
    "    if(test_result>0.88):\n",
    "        result_list.append(test_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817\n"
     ]
    }
   ],
   "source": [
    "result_list_np = np.array(result_list)\n",
    "result_list_n = result_list_np.sum(axis=0)\n",
    "r = [int(item>=19) for item in result_list_n]\n",
    "print(accuracy_score(r,test_label_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
